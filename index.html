<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Prajval Kumar Murali</title>
    <meta name="author" content="Prajval Murali">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <meta name="google-site-verification" content="ZvkJKAtKTqWNzI9OJ-WDKIc9MSRyhPTvACMnStdR9aE"/>
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Prajval Kumar Murali</name>
                        </p>

                        <p> I am currently a Ph.D. doctoral candidate working towards my dissertation on Active Visuo-Tactile
                            Perception
                            for Manipulation and Human-Robot Interaction at the <a
                                    href="https://www.bmwgroup.com/en.html">BMW Group</a> and with <a
                                    href="https://www.gla.ac.uk/">University of Glasgow</a>.

                        </p>

                        <p>
                            My doctoral research focuses on developing novel methodologies for multi-modal vision and tactile
                            perception. Particularly for robust and active object pose estimation for robotics in unstructured challenging scenarios  using vision and tactile
                            perception. Furthermore, my research also focuses on leveraging the complementary information available
                            from each modality to perform cross-modal perception.

                        </p>
                        <p>
                            Previously, I have worked as a robotics engineer at the <a href="https://iit.it/">Italian Institute of
                            Technology</a> from 2018 to 2020.
                            Here, I worked in a joint lab with <a href="https://www.dca.it/en/">Danieli Automation</a>
                            towards reducing risks related to occupational health hazards and thereby increasing safety
                            for workers in the steel industry
                            through human-robot collaboration.
                        </p>
                        <p>
                            I graduated my M.S. degree with highest distinction in the <a
                                href="https://master-emaro.ec-nantes.fr/">European Masters on Advanced Robotics
                            (EMARO+)</a>, an Erasmus Mundus Master’s program at
                            <a href="https://www.ec-nantes.fr/">Ecole Centrale de Nantes</a>, France and <a
                                href="https://unige.it/it/">University of Genova</a>, Italy in 2018.
                            I worked on my Master's thesis on Human-Robot Collaboration in a manufacturing environment
                            at the <a href="https://www.schaeffler.com/fork/">Schaeffler Group</a>.
                            I have been awarded the prestigious Erasmus Mundus Consortium Scholarship by the European
                            Union for my master's program.
                            I earned my bachelor's degree in engineering with highest distinctions in 2016 from the <a
                                href="https://www.nitt.edu/">National Institute of Technology Tiruchirappalli</a>,
                            India.
                        <p>
                            <strong> Expertise: </strong> Robotics / deep learning / multi-modal perception / point
                            cloud registration / 3D
                            reconstruction / robot-sensor calibration / 3D computer vision / tactile sensing


                        </p>
                        <strong> Disclaimer: </strong> This is my personal website and opinions presented here are my
                        own and do not reflect the views of my employers.
                        <p>

                        </p>
                        <p style="text-align:center">
                            <a href="https://scholar.google.com/citations?user=8W7cVy4AAAAJ&hl=en">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://github.com/prajval10">Github</a> &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/prajval-kumar-murali/">LinkedIn</a> &nbsp/&nbsp
                            <a href="https://orcid.org/0000-0002-7330-7906">ORCID</a>

                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/profile-circle1.png"><img style="width:100%;max-width:100%" alt="profile photo"
                                                                  src="images/profile-circle1.png"
                                                                  class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- NEWS -->
<!--            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--                        <h2>News</h2>-->
<!--                        <p>-->
<!--                            <strong>[Feb, 2023]</strong> New paper on "GMCR: Graph-based Maximum Consensus Estimation-->
<!--                            for Point Cloud Registration" has been accepted to IEEE ICRA 2023 </a>-->
<!--                        </p>-->
<!--                        <p><strong>[July, 2022]</strong> Our paper wins best paper award at IEEE FLEPS 2022  </a>-->
<!--                        </p>-->
<!--                    </td>-->
<!--                </tr>-->
<!--                </tbody>-->
<!--            </table>-->

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h1> Projects </h1>
                        <h2>Awards</h2>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- AQWARDs -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
<!--                        <h2>Awards</h2>-->

                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:200px;max-width:500px" src="images/best-paper-award-new.png"
                            class="center"></td>
                    <td style="padding:120px;width:25%;vertical-align:middle">

                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Active Multi-Modal Pose Estimation</h2>
                    </td>
                </tr>
                </tbody>
            </table>



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr onmouseout="font_stop()" onmouseover="font_start()">


                    <!-- ICRA-RA-L 2022 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/ral-icra.gif"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Active Visuo-Tactile Interactive Robotic Perception for Accurate Object Pose
                            Estimation in Dense Clutter
                        </papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong></u>, Anirvan Dutta, Michael Gentner, Etienne Burdet,
                        Ravinder Dahiya and Mohsen Kaboli
                        <br>
                        <em>IEEE Robotics and Automation Letters (IEEE RA-L)</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/9709520">[paper]</a><a
                            href=" https://youtu.be/sjqWRFLL2Xw">[video]</a>
                    </td>
                </tr>

                <!-- IROS 2021 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/iros_website.gif"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Active Visuo-Tactile Point Cloud Registration for Accurate Pose Estimation of
                            Objects in an Unknown Workspace
                        </papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong></u>, Michael Gentner, and Mohsen Kaboli
                        <br>
                        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2021</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/document/9636877">[paper]</a> <a
                            href="https://youtu.be/Pu0Ghjlff7Q">[video]</a>
                    </td>
                </tr>

                <!-- FLEPS 2022 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/bunny_sim_final (1).png"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>An Empirical Evaluation of Various Information Gain Criteria for Active Tactile
                            Action Selection for Pose Estimation
                        </papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong></u>, Ravinder Dahiya and Mohsen Kaboli
                        <br>
                        <em>The IEEE Int. Conf on Flexible and Printable Sensors and Systems (FLEPS 2022)</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/document/9781598">[paper]</a>
                        <br>
                        <strong style="color:red;">Best Paper Award Winner</strong>
                    </td>
                </tr>

                <!-- ICRA 2023 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/icra_23.gif"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>GMCR: Graph-based Maximum Consensus Estimation for Point Cloud Registration
                        </papertitle>
                        <br>
                        Michael Gentner, <u><strong>Prajval Kumar Murali</strong></u>, and Mohsen Kaboli
                        <br>
                        <em>The IEEE International Conference on Robotics and Automation (ICRA 2023)</em>
                        <br>
                    </td>
                </tr>

                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Robust Cross-Modal Perception</h2>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <!-- IROS-RA-L 2022 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/ral_iros22.gif"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Deep Active Cross-Modal Visuo-Tactile Transfer Learning for Robotic Object
                            Recognition
                        </papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong></u>, Cong Wang, Dongheui Lee, Ravinder Dahiya and
                        Mohsen Kaboli
                        <br>
                        <em>IEEE Robotics and Automation Letters (IEEE RA-L)</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9830870">[paper]</a>
                    </td>
                </tr>


                <!-- FLEPS 2022 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/fig1.png"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Towards Robust 3D Object Recognition with Dense-to-Sparse Deep Domain Adaptation
                        </papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong> </u>, Cong Wang, Ravinder Dahiya and Mohsen Kaboli
                        <br>
                        <em>The IEEE Int. Conf on Flexible and Printable Sensors and Systems (FLEPS 2022)</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/document/9781490">[paper]</a>
                    </td>
                </tr>

                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Human-Robot/ Human-Vehicle Collaboration</h2>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>


                <!-- AIS -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/figure1_new.png"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Intelligent In-Vehicle Interaction Technologies</papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong> </u>, Mohsen Kaboli, and Ravinder Dahiya
                        <br>
                        <em>Advanced Intelligent Systems</em>
                        <br>
                        <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/aisy.202100122">[paper]</a>
                    </td>
                </tr>

                <!-- JIST 2020 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/jist.gif"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Deployment and Evaluation of a Flexible Human-Robot Collaboration Model Based on
                            AND/OR Graphs in a Manufacturing Environment
                        </papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong></u>, Kourosh Darvish, Fulvio Mastrogiovanni
                        <br>
                        <em>Springer Journal of Intelligent Service Robotics (JIST)</em>
                        <br>
                        <a href="https://link.springer.com/article/10.1007/s11370-020-00332-9">[paper]</a><a
                            href="https://www.youtube.com/watch?v=mF22PPmwHN0&feature=youtu.be">[video]</a><a
                            href="https://github.com/EmaroLab/industrialRobot_task_planning">[code]</a>
                    </td>
                </tr>

                <!-- ETFA 2020 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/etfa.png"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>A Flexible Software Architecture for Robotic Industrial Applications</papertitle>
                        <br>
                        Angelo Rendiniello, Alberto Remus, Ines Sorrentino,<u><strong> Prajval Kumar
                        Murali </strong></u>, Daniele Pucci, Marco Maggiali, Lorenzo Natale, Enrico Villagrossi, Andrea
                        Polo, Alessandro Ardesi, Silvio Traversaro
                        <br>
                        <em>IEEE International Conference on Emerging Technologies and Factory Automation, ETFA
                            2020</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/9212095">[paper]</a>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Multi-Modal Sensor Calibration</h2>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <!-- ICRA 2021 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/icub.png"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>In Situ Translational Hand-Eye Calibration of Laser Profile Sensors using Arbitrary
                            Objects
                        </papertitle>
                        <br>
                        <u><strong>Prajval Kumar Murali</strong></u>, Ines Sorrentino, Angelo Rendiniello, Claudio
                        Fantacci, Enrico Villagrossi, Andrea Polo, Alessandro Ardesi, Marco Maggiali, Lorenzo Natale,
                        Daniele Pucci, Silvio Traversaro
                        <br>
                        <em>IEEE International Conference on Robotics and Automation, ICRA 2021</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/iel7/9560720/9560666/09561055.pdf">[paper]</a>
                    </td>
                </tr>

                <!-- IROS 2015 -->
                <tr onmouseout="font_stop()" onmouseover="font_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle"><img
                            style="max-height:180px;max-width:180px" src="images/iros.gif"></td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Active calibration of tactile sensors mounted on a robotic hand</papertitle>
                        <br>
                        Benjamin Navarro, <u><strong>Prajval Kumar Murali </strong></u>, Aicha Fonte, Philippe Fraisse,
                        Gérard Poisson, Andrea Cherubini
                        <br>
                        <em>IEEE/RSJ Intelligent RObots and Systems IROS 2015, Workshop on Multimodal sensor-based robot
                            control for HRI and soft manipulation</em>
                        <br>
                        <a href="https://hal-lirmm.ccsd.cnrs.fr/lirmm-01247148/document">[paper]</a><a
                            href="https://www.youtube.com/watch?v=kJm6jaVxBlI">[video]</a>
                        <br>
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Courtesy -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <p style="text-align:right;font-size:small;">
                            Website by the courtesy of <a href="https://jonbarron.info/">Jon Barron</a>.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>


</html>
